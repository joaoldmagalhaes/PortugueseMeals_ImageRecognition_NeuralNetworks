{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e61956af",
      "metadata": {
        "id": "e61956af"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<center> <h1> PORTUGUESE MEALS IMAGE RECOGNITION </h1> </center> <br>\n",
        "<center> DEEP LEARNING NEURAL NETWORKS | FALL SEMESTER 2022/ 2023 </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pAW032SuLQAv",
      "metadata": {
        "id": "pAW032SuLQAv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c35ae39d",
      "metadata": {
        "id": "c35ae39d"
      },
      "source": [
        "**GROUP 10:** <br>\n",
        "\n",
        "- João Magalhães      `20211044` <br>\n",
        "- Maria Trindade      `20211049` <br>\n",
        "- Nuno Bolas          `20211052` <br>\n",
        "- Jorge Gaspar        `20211057` <br> \n",
        "- Mariana Teixeira    `20211058` <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a07a871",
      "metadata": {
        "id": "0a07a871"
      },
      "source": [
        "<a id='toc'></a>\n",
        "\n",
        "\n",
        "**Table of Contents** <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a149aa4",
      "metadata": {
        "id": "4a149aa4"
      },
      "source": [
        "[BACK TO TOC](#toc)\n",
        "    \n",
        "<a id='import'></a>\n",
        "\n",
        "# <font color = 'darkblue'> 1. Import</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555260b7",
      "metadata": {
        "id": "555260b7"
      },
      "source": [
        "<a class=\"anchor\" id=\"importlibraries\">\n",
        "\n",
        "## 1.1. Import Libraries\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c292e9",
      "metadata": {
        "id": "56c292e9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import Sequential, layers, initializers, regularizers, optimizers, metrics \n",
        "from keras import callbacks\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "import io\n",
        "import itertools\n",
        "from packaging import version\n",
        "import random \n",
        "import zipfile\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "import PIL.Image\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6eab5a4",
      "metadata": {
        "id": "d6eab5a4"
      },
      "outputs": [],
      "source": [
        "# Set the style for the Seaborn's plots\n",
        "sns.set_style('whitegrid',{\n",
        "    'xtick.bottom': False,\n",
        "    'xtick.color': '.1',\n",
        "    'xtick.direction': 'out',\n",
        "    'xtick.top': False,\n",
        "    'xtick.major.size': 1,\n",
        "    'xtick.minor.size': 0.5,\n",
        "    'ytick.left': True,\n",
        "    'ytick.color': '.1',\n",
        "    'ytick.direction': 'out',\n",
        "    'ytick.right': False,\n",
        "    'ytick.major.size': 1,\n",
        "    'ytick.minor.size': 0.5,    \n",
        "    'ytick.color': '.1',\n",
        "    'grid.linestyle': '--',\n",
        "    'axes.edgecolor': '.1',\n",
        "    'grid.color': '0.8'\n",
        " })\n",
        "\n",
        "palette = sns.color_palette(\"Set2\") \n",
        "sns.set_palette(palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b95980",
      "metadata": {
        "id": "f9b95980"
      },
      "source": [
        "<a class=\"anchor\" id=\"setuptensorboard\">\n",
        "\n",
        "## 1.2. Setup Tensorboard\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3520e358",
      "metadata": {
        "id": "3520e358"
      },
      "source": [
        "Code inspired by: https://neptune.ai/blog/tensorboard-tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a24ea0",
      "metadata": {
        "id": "17a24ea0"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b204c5",
      "metadata": {
        "id": "c6b204c5"
      },
      "outputs": [],
      "source": [
        "#Folder to save the logs\n",
        "log_folder = 'logs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d5b384",
      "metadata": {
        "id": "45d5b384"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d58f802b",
      "metadata": {
        "id": "d58f802b"
      },
      "outputs": [],
      "source": [
        "#Clear previous logs\n",
        "\n",
        "#collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "#jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "#try:\n",
        "#    shutil.rmtree('logs')\n",
        "#except:\n",
        "#    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120c172b",
      "metadata": {
        "id": "120c172b"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf17fde",
      "metadata": {
        "id": "ebf17fde"
      },
      "source": [
        "[BACK TO TOC](#toc)\n",
        "    \n",
        "<a id='dataunderstanding'></a>\n",
        "\n",
        "# <font color = 'darkblue'> 2. Data Understanding and Preprocessing </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e077650",
      "metadata": {
        "id": "0e077650"
      },
      "source": [
        "<a class=\"anchor\" id=\"loaddata\">\n",
        "\n",
        "## 2.1. Data Loading\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3cb1f16",
      "metadata": {
        "id": "c3cb1f16"
      },
      "outputs": [],
      "source": [
        "#Defining files path\n",
        "\n",
        "path = '../Dataset/dataset_dishes'\n",
        "\n",
        "try: \n",
        "    os.chdir(path)\n",
        "except: \n",
        "    print('Curret path is: ' + os.getcwd())\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2983b7",
      "metadata": {
        "id": "6a2983b7"
      },
      "source": [
        "Download the dataset of the dishes photos following the tensorflow tutorial for loading images https://www.tensorflow.org/tutorials/load_data/images and the notebook shown in class (t_w3_2022_drive)\n",
        "\n",
        "The dataset contains 6 subdirectories corresponding to each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed4e21d",
      "metadata": {
        "id": "0ed4e21d"
      },
      "outputs": [],
      "source": [
        "data_dir = os.getcwd()\n",
        "data_dir_train = pathlib.Path(os.path.join(data_dir, \"training\"))\n",
        "data_dir_val = pathlib.Path(os.path.join(data_dir, \"validation\"))\n",
        "data_dir_test= pathlib.Path(os.path.join(data_dir, \"test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd7de1f",
      "metadata": {
        "id": "7fd7de1f"
      },
      "outputs": [],
      "source": [
        "data_dir_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91aa7462",
      "metadata": {
        "id": "91aa7462"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "print(image_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da89df03",
      "metadata": {
        "id": "da89df03"
      },
      "source": [
        "Create a dataframe and check the classes' balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e55441ae",
      "metadata": {
        "id": "e55441ae"
      },
      "outputs": [],
      "source": [
        "filepaths = list(data_dir_train.glob(r'**/*.jpg')) #iterate over folders and sub-folders\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths)) #Create labels with the number of classes\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str) #create filepaths column\n",
        "labels = pd.Series(labels, name='Label') #create classes names column\n",
        "\n",
        "df = pd.concat([filepaths, labels], axis=1) #create dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7711bb9",
      "metadata": {
        "id": "f7711bb9"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f426d6",
      "metadata": {
        "id": "22f426d6"
      },
      "outputs": [],
      "source": [
        "df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0338b8cb",
      "metadata": {
        "id": "0338b8cb"
      },
      "source": [
        "Since not every class has the same number of images, we need to sample them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288cdcf9",
      "metadata": {
        "id": "288cdcf9"
      },
      "outputs": [],
      "source": [
        "# category_samples = []\n",
        "# for category in df['Label'].unique():\n",
        "#     category_slice = df.query(\"Label == @category\")\n",
        "#     category_samples.append(category_slice.sample(100, random_state=1))\n",
        "# df = pd.concat(category_samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8000ee",
      "metadata": {
        "id": "7e8000ee"
      },
      "outputs": [],
      "source": [
        "# df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97019b7",
      "metadata": {
        "id": "c97019b7"
      },
      "outputs": [],
      "source": [
        "# Falta fazer o exploration do dataset original, eliminar as classes que não queremos e juntar o scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31242b52",
      "metadata": {
        "id": "31242b52"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a8e3e98e",
      "metadata": {
        "id": "a8e3e98e"
      },
      "source": [
        "Create the dataset with the images: Start by loading the images and spliting the dataset in training and validation using the ``tf.keras.utils.image_dataset_from_directory`` function and the validation_split=0.2 argument like shown in class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015c9968",
      "metadata": {
        "id": "015c9968"
      },
      "outputs": [],
      "source": [
        "# Define list of parameters:\n",
        "image_size=(128, 128) #image size (height and width)\n",
        "crop_to_aspect_ratio = True #respects the images aspect ratio when resizing. output image is cropped to fit the target aspect ratio if different\n",
        "color_mode='rgb' #default color\n",
        "batch_size=96 #no. of images loaded in each batch\n",
        "shuffle=True #randomization of the instances so each batch has the representation of the maximum no. of classes\n",
        "seed=28 #seed used for the shuffle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e516c0a",
      "metadata": {
        "id": "5e516c0a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create the train and validation datasets (80% for training and 20% validation):\n",
        "data_dir_train = '/content/drive/MyDrive/IMS_DeepLearning/dataset_dishes/training'\n",
        "data_dir_val = '/content/drive/MyDrive/IMS_DeepLearning/dataset_dishes/validation'\n",
        "\n",
        "ds_train = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "    image_size=image_size,\n",
        "    crop_to_aspect_ratio=crop_to_aspect_ratio,\n",
        "    color_mode=color_mode,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=shuffle,\n",
        "    seed=seed)\n",
        "\n",
        "ds_val = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir_val,\n",
        "  image_size=image_size,\n",
        "    crop_to_aspect_ratio=crop_to_aspect_ratio,\n",
        "    color_mode=color_mode,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=shuffle,\n",
        "    seed=seed)\n",
        "\n",
        "class_names = ds_train.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903df373",
      "metadata": {
        "id": "903df373"
      },
      "outputs": [],
      "source": [
        "# Check object properties\n",
        "print(\"\\nObject's type:\\t\", type(ds_train))\n",
        "print(\"Is it a tf.data.Dataset?\\t R:\",isinstance(ds_train, tf.data.Dataset))\n",
        "print(\"Classes:\", ds_train.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c94d883",
      "metadata": {
        "id": "6c94d883"
      },
      "source": [
        "Checking the training dataset first nine images to get a feeling of the images' quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7cacd0",
      "metadata": {
        "id": "ad7cacd0"
      },
      "outputs": [],
      "source": [
        "#Code source: https://www.tensorflow.org/tutorials/load_data/images\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in ds_train.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e01bfa",
      "metadata": {
        "id": "40e01bfa"
      },
      "source": [
        "Checking the number of images per class of the training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c393421a",
      "metadata": {
        "id": "c393421a"
      },
      "outputs": [],
      "source": [
        "classes = []\n",
        "for (image,label) in tuple(ds_train.unbatch()):\n",
        "    classes.append(label.numpy())\n",
        "classes = pd.Series(classes)\n",
        "count = classes.value_counts()\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc613498",
      "metadata": {
        "id": "dc613498"
      },
      "outputs": [],
      "source": [
        "classes = []\n",
        "for (image,label) in tuple(ds_val.unbatch()):\n",
        "    classes.append(label.numpy())\n",
        "classes = pd.Series(classes)\n",
        "count = classes.value_counts()\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4881c4a6",
      "metadata": {
        "id": "4881c4a6"
      },
      "source": [
        "Notice the training and validation datasets are imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8867b6c",
      "metadata": {
        "id": "b8867b6c"
      },
      "outputs": [],
      "source": [
        "for batch_x_train, batch_y_train in ds_train:\n",
        "    print(batch_x_train.shape, batch_y_train.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dw-QV8j5cGHL",
      "metadata": {
        "id": "Dw-QV8j5cGHL"
      },
      "outputs": [],
      "source": [
        "for batch_x_val, batch_y_val in ds_val:\n",
        "    print(batch_x_val.shape, batch_y_val.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd7bc7ad",
      "metadata": {
        "id": "cd7bc7ad"
      },
      "source": [
        "batch_x_train is a tensor with a batch of 64 images of shape 128x128 and color channel RGB 3. batch_y_train is a tensor with 64 images and corresponding labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccc5386c",
      "metadata": {
        "id": "ccc5386c"
      },
      "source": [
        "<a class=\"anchor\" id=\"Datapreprocessing\">\n",
        "\n",
        "## 2.2. Data Preprocessing\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb0e4218",
      "metadata": {
        "id": "cb0e4218"
      },
      "source": [
        "Encoding the classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xYIUyvt6X20A",
      "metadata": {
        "id": "xYIUyvt6X20A"
      },
      "outputs": [],
      "source": [
        "encoding = layers.CategoryEncoding(num_tokens=6, output_mode=\"one_hot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqjzk01rYEGN",
      "metadata": {
        "id": "oqjzk01rYEGN"
      },
      "outputs": [],
      "source": [
        "ds_train_encoded = ds_train.map(lambda x, y: (x, encoding(y)))\n",
        "batch_x_train_encoded, batch_y_train_encoded = next(iter(ds_train_encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_WJZgox8YiGO",
      "metadata": {
        "id": "_WJZgox8YiGO"
      },
      "outputs": [],
      "source": [
        "ds_val_encoded = ds_val.map(lambda x, y: (x, encoding(y)))\n",
        "batch_x_val_encoded, batch_y_val_encoded = next(iter(ds_val_encoded))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82bf6b3f",
      "metadata": {
        "id": "82bf6b3f"
      },
      "source": [
        "Rescaling the train and validation datatsets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02246238",
      "metadata": {
        "id": "02246238"
      },
      "outputs": [],
      "source": [
        "rescaling = layers.Rescaling(1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90298a3",
      "metadata": {
        "id": "e90298a3"
      },
      "outputs": [],
      "source": [
        "ds_train_scaled = ds_train_encoded.map(lambda x, y: (rescaling(x), y))\n",
        "batch_x_train_scaled, batch_y_train_scaled = next(iter(ds_train_scaled))\n",
        "\n",
        "\n",
        "print(np.min(batch_x_train_scaled), np.max(batch_x_train_scaled))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0afe5487",
      "metadata": {
        "id": "0afe5487"
      },
      "outputs": [],
      "source": [
        "ds_val_scaled = ds_val_encoded.map(lambda x, y: (rescaling(x), y))\n",
        "batch_x_val_scaled, batch_y_val_scaled = next(iter(ds_val_scaled))\n",
        "\n",
        "\n",
        "print(np.min(batch_x_val_scaled), np.max(batch_x_val_scaled))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014367d1",
      "metadata": {
        "id": "014367d1"
      },
      "source": [
        "<a class=\"anchor\" id=\"Dataaugmentation\">\n",
        "\n",
        "### *2.2.1 Data Augmentation*\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84368bca",
      "metadata": {
        "id": "84368bca"
      },
      "source": [
        "Data augmentation techniques allow to include diversity and increase quality of the training dataset without the need for further data through the random transformations. Since the dataset used is small, data augmentation is particularly important to train the model.<br>\n",
        "Following the notebook solved in class, we defined a data augmentation pipeline with the layers: \n",
        "-  `tf.keras.layers.RandomFlip`\n",
        "\n",
        "-  `tf.keras.layers.RandomTranslation`\n",
        "\n",
        "-  `tf.keras.layers.RandomRotation`\n",
        "\n",
        "-  `tf.keras.layers.RandomZoom`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "625ac81b",
      "metadata": {
        "id": "625ac81b"
      },
      "outputs": [],
      "source": [
        "augmentation = Sequential([layers.RandomFlip(seed=seed), \n",
        "                           layers.RandomRotation(0.1, seed=seed), \n",
        "                           layers.RandomZoom(0.1, seed=seed),\n",
        "                           layers.RandomTranslation(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1))], \n",
        "                           name=\"augmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b09dffe",
      "metadata": {
        "id": "3b09dffe"
      },
      "source": [
        "Showing a sample image with the data augmentation pipeline applied (as the layers defined only implement data augmentation during the training, to test it, the training must be set to True):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f217af1",
      "metadata": {
        "id": "4f217af1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(tf.cast(augmentation(batch_x_train[0], training=True), tf.int32)) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cbb86a8",
      "metadata": {
        "id": "4cbb86a8"
      },
      "source": [
        "Using the function defined in class to visualize the pipeline defined applied on a set of images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613978b0",
      "metadata": {
        "id": "613978b0"
      },
      "outputs": [],
      "source": [
        "def show_sample_batch(ds, augmentation, grid_size=(4, 4), figsize=(14, 10)):\n",
        "    n_images = grid_size[0]\n",
        "    # Get a batch via iteration\n",
        "    iter_ = iter(ds)\n",
        "    batch_x, batch_y = iter_.next()\n",
        "    batch_x, batch_y = batch_x[0:n_images], batch_y[0:n_images]\n",
        "    # Plot the images\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    count = 0\n",
        "    for i, (img, y) in enumerate(zip(batch_x, batch_y)):\n",
        "        img_ = tf.cast(img, tf.int32)\n",
        "        for j in range(grid_size[1]):\n",
        "            # Prepare the image\n",
        "            if j>0:\n",
        "                img_ = tf.cast(augmentation(img_, training=True), tf.int32)\n",
        "            ax = plt.subplot(grid_size[0], grid_size[1],  count + 1)        \n",
        "            plt.imshow(img_)\n",
        "            plt.title(\"{} {} image of class \\\"{}\\\"\".format(\n",
        "                img.shape, \"original\" if j==0 else \"augmented\", y), size=6)\n",
        "            plt.axis(\"off\")\n",
        "            count+=1\n",
        "\n",
        "show_sample_batch(ds_train, augmentation=augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa765f0",
      "metadata": {
        "id": "9fa765f0"
      },
      "source": [
        "[BACK TO TOC](#toc)\n",
        "    \n",
        "<a id='modelling'></a>\n",
        "\n",
        "# <font color = 'darkblue'> 3. Modelling </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A_Tl191TpquY",
      "metadata": {
        "id": "A_Tl191TpquY"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv1\">\n",
        "\n",
        "## 3.0. Pre-modelling\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da4b94aa",
      "metadata": {
        "id": "da4b94aa"
      },
      "source": [
        "Configure dataset for performance (https://www.tensorflow.org/tutorials/load_data/images):\n",
        "- `Dataset.cache` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model.\n",
        "- `Dataset.prefetch`overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cfcca8",
      "metadata": {
        "id": "c9cfcca8"
      },
      "outputs": [],
      "source": [
        "# AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# ds_train = ds_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# ds_val = ds_val.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NM-sirjM-VGL",
      "metadata": {
        "id": "NM-sirjM-VGL"
      },
      "outputs": [],
      "source": [
        "#Shared parameters:\n",
        "epochs=100\n",
        "batch_size=96\n",
        "suffle= True\n",
        "input_shape = batch_x_train_scaled.shape\n",
        "n_classes=6\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9892d89",
      "metadata": {
        "id": "e9892d89"
      },
      "source": [
        "Calculate the evaluation weights for each class (since the dataset is no balanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b31c0fe",
      "metadata": {
        "id": "7b31c0fe"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(batch_y_train.numpy()), y= batch_y_train.numpy())\n",
        "\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2821d053",
      "metadata": {
        "id": "2821d053"
      },
      "outputs": [],
      "source": [
        "weights= dict(enumerate(class_weights.flatten(), 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vlHCQIGfKV-r",
      "metadata": {
        "id": "vlHCQIGfKV-r"
      },
      "outputs": [],
      "source": [
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ON5ONHM2Mncr",
      "metadata": {
        "id": "ON5ONHM2Mncr"
      },
      "outputs": [],
      "source": [
        "val_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(batch_y_val.numpy()), y= batch_y_val.numpy())\n",
        "\n",
        "val_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MAQwGEgarZGV",
      "metadata": {
        "id": "MAQwGEgarZGV"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35cb0639",
      "metadata": {
        "id": "35cb0639"
      },
      "source": [
        "Define function for confusion matrix inspired by: https://neptune.ai/blog/tensorboard-tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d751a9f",
      "metadata": {
        "id": "9d751a9f"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "    threshold = cm.max() / 2.\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    return figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f77d133a",
      "metadata": {
        "id": "f77d133a"
      },
      "outputs": [],
      "source": [
        "def plot_to_image(figure):\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "\n",
        "    digit = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    digit = tf.expand_dims(digit, 0)\n",
        "\n",
        "    return digit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba17414",
      "metadata": {
        "id": "aba17414"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv1\">\n",
        "\n",
        "## 3.1. CNN_V1\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59726d89",
      "metadata": {
        "id": "59726d89"
      },
      "outputs": [],
      "source": [
        "#Clear previous logs\n",
        "\n",
        "#collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "#jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "#try:\n",
        "#    shutil.rmtree('logs')\n",
        "#except:\n",
        "#    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b45d42",
      "metadata": {
        "id": "a8b45d42"
      },
      "outputs": [],
      "source": [
        "# Architecture v1\n",
        "cnn1 = Sequential([# Feature extraction                   \n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   # Classification \n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "\n",
        "# Builds the DAG (comment if input_shape was already provided to the first layer)\n",
        "cnn1.build(input_shape)\n",
        "# Check network\n",
        "cnn1.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0858a27b",
      "metadata": {
        "id": "0858a27b"
      },
      "outputs": [],
      "source": [
        "model=cnn1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ngj73YqpO3nP",
      "metadata": {
        "id": "Ngj73YqpO3nP"
      },
      "outputs": [],
      "source": [
        "cnn1.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy']) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RVF08QdraD5e",
      "metadata": {
        "id": "RVF08QdraD5e"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef78f66a",
      "metadata": {
        "id": "ef78f66a"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn1.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d2f0c0",
      "metadata": {
        "id": "11d2f0c0"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab39e7d",
      "metadata": {
        "id": "9ab39e7d"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ae9f13",
      "metadata": {
        "id": "e2ae9f13"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7124027",
      "metadata": {
        "id": "b7124027"
      },
      "outputs": [],
      "source": [
        "# Model training (v1)\n",
        "#Adding early stopping to help choosing the best number of epochs\n",
        "#code from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
        "\n",
        "#Train\n",
        "history1 = cnn1.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8740791f",
      "metadata": {
        "id": "8740791f"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5edc5c9e",
      "metadata": {
        "id": "5edc5c9e"
      },
      "source": [
        "Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd572dc",
      "metadata": {
        "id": "ccd572dc"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist1 = pd.DataFrame.from_dict(history1.history)\n",
        "df_hist1[\"Epoch\"] = np.arange(1, len(df_hist1) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist1.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81974e5f",
      "metadata": {
        "id": "81974e5f"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv2\">\n",
        "\n",
        "## 3.2. CNN_V2 , Batch Normalization\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a698d56",
      "metadata": {
        "id": "2a698d56"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oRgd0I-8z7Q8",
      "metadata": {
        "id": "oRgd0I-8z7Q8"
      },
      "source": [
        "### stabilizing input features of a deep network.\n",
        "From [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, by Sergey Ioffe, Christian Szegedy](https://arxiv.org/abs/1502.03167):\n",
        "\n",
        "    \"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.\"\n",
        "\n",
        "\n",
        "Adds ``BatchNormalization()`` layers before applying the non-linearities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29YKs90qybC5",
      "metadata": {
        "id": "29YKs90qybC5"
      },
      "outputs": [],
      "source": [
        "# Architecture v2\n",
        "cnn2_1 = Sequential([# The batch normalization layer\n",
        "                   #layers.Input(input_shape[1:]), \n",
        "                   layers.BatchNormalization(),                   \n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3), #input_shape=input_shape[1:],\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.BatchNormalization(),   \n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "\n",
        "# Builds the DAG (comment if input_shape was already provided to the first layer)\n",
        "cnn2_1.build(input_shape)\n",
        "# Check network\n",
        "cnn2_1.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c877dcd2",
      "metadata": {
        "id": "c877dcd2"
      },
      "outputs": [],
      "source": [
        "model=cnn2_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "odCe6_Xoh412",
      "metadata": {
        "id": "odCe6_Xoh412"
      },
      "outputs": [],
      "source": [
        "cnn2_1.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bce88f",
      "metadata": {
        "id": "36bce88f"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn2_1.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407509cf",
      "metadata": {
        "id": "407509cf"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd8ba61",
      "metadata": {
        "id": "dfd8ba61"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ffffcd2",
      "metadata": {
        "id": "1ffffcd2"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UfZPi-sLh413",
      "metadata": {
        "id": "UfZPi-sLh413"
      },
      "outputs": [],
      "source": [
        "# Model training (v2)\n",
        "\n",
        "\n",
        "#Train\n",
        "history2_1 = cnn2_1.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5191ae81",
      "metadata": {
        "id": "5191ae81"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SU7sP0Glh413",
      "metadata": {
        "id": "SU7sP0Glh413"
      },
      "source": [
        "Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kP3Vp3Bsh413",
      "metadata": {
        "id": "kP3Vp3Bsh413"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist2_1 = pd.DataFrame.from_dict(history2_1.history)\n",
        "df_hist2_1[\"Epoch\"] = np.arange(1, len(df_hist2_1) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist2_1.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a_Uty-Oe0J7u",
      "metadata": {
        "id": "a_Uty-Oe0J7u"
      },
      "source": [
        "### BatchNormalization() layer to the head of the Sequential() model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1280d20",
      "metadata": {
        "id": "c1280d20"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pHWEesN_h412",
      "metadata": {
        "id": "pHWEesN_h412"
      },
      "outputs": [],
      "source": [
        "# Architecture v2.2\n",
        "cnn2_2 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),     \n",
        "                   #layers.Input(input_shape[1:]),\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3), #input_shape=input_shape[1:],\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   # Classification (use units=n_classes, activation=\"softmax\" for multi-class problems)\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "\n",
        "# Builds the DAG (comment if input_shape was already provided to the first layer)\n",
        "cnn2_2.build(input_shape)\n",
        "# Check network\n",
        "cnn2_2.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425df6e0",
      "metadata": {
        "id": "425df6e0"
      },
      "outputs": [],
      "source": [
        "model=cnn2_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iQ6X5cLNyoEH",
      "metadata": {
        "id": "iQ6X5cLNyoEH"
      },
      "outputs": [],
      "source": [
        "cnn2_2.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa6afd0c",
      "metadata": {
        "id": "aa6afd0c"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d244de",
      "metadata": {
        "id": "f5d244de"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn2_2.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2581a406",
      "metadata": {
        "id": "2581a406"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0833efb7",
      "metadata": {
        "id": "0833efb7"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f316a651",
      "metadata": {
        "id": "f316a651"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ag-ylzqByoEI",
      "metadata": {
        "id": "ag-ylzqByoEI"
      },
      "outputs": [],
      "source": [
        "# Model training (v2)\n",
        "\n",
        "#Train\n",
        "\n",
        "history2_2 = cnn2_2.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c60475",
      "metadata": {
        "id": "62c60475"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FvuT3ho7yoEJ",
      "metadata": {
        "id": "FvuT3ho7yoEJ"
      },
      "source": [
        "Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1gG8JUOiyoEJ",
      "metadata": {
        "id": "1gG8JUOiyoEJ"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist2_2 = pd.DataFrame.from_dict(history2_2.history)\n",
        "df_hist2_2[\"Epoch\"] = np.arange(1, len(df_hist2_2) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist2_2.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744709bc",
      "metadata": {
        "id": "744709bc"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv3\">\n",
        "\n",
        "## 3.3. CNN_V3, data augmentation\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268baf54",
      "metadata": {
        "id": "268baf54"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb90f62",
      "metadata": {
        "id": "1fb90f62"
      },
      "outputs": [],
      "source": [
        "for layer in augmentation.layers:\n",
        "    print(layer.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5db198b",
      "metadata": {
        "id": "a5db198b"
      },
      "outputs": [],
      "source": [
        "# Architecture v3\n",
        "cnn3 = Sequential([# Add data augmentation\n",
        "                   augmentation,                         \n",
        "                   # Feature extraction\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   # Classification (use units=n_classes, activation=\"softmax\" for multi-class)\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG (comment if input_shape was already provided to the first layer)\n",
        "cnn3.build(input_shape)\n",
        "# Check network\n",
        "cnn3.summary()  # alternatively use tf.keras.utils.plot_model(cnn1, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea44daa",
      "metadata": {
        "id": "5ea44daa"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn3.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a862fa",
      "metadata": {
        "id": "f4a862fa"
      },
      "outputs": [],
      "source": [
        "model=cnn3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d281da3",
      "metadata": {
        "id": "9d281da3"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9118868",
      "metadata": {
        "id": "d9118868"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn3.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f2e660",
      "metadata": {
        "id": "72f2e660"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81072343",
      "metadata": {
        "id": "81072343"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "978e50a9",
      "metadata": {
        "id": "978e50a9"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe6d1ce",
      "metadata": {
        "id": "afe6d1ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Train\n",
        "history3 = cnn3.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03fb9962",
      "metadata": {
        "id": "03fb9962"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e20a2e8",
      "metadata": {
        "id": "7e20a2e8"
      },
      "outputs": [],
      "source": [
        "df_hist3 = pd.DataFrame.from_dict(history3.history)\n",
        "df_hist3[\"Epoch\"] = np.arange(1, len(df_hist3) + 1, 1)\n",
        "# 4.\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist3.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                  secondary_y = secondary_y,\n",
        "                  kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                  ylabel=\"Cross-entropy\", \n",
        "                  xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                  color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.right_ax.set_ylim(0.45, 1.05)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed47742f",
      "metadata": {
        "id": "ed47742f"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv4\">\n",
        "\n",
        "## 3.4. CNN_V4 , without pre-processing\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceccbbc7",
      "metadata": {
        "id": "ceccbbc7"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9dc7c46",
      "metadata": {
        "id": "b9dc7c46"
      },
      "outputs": [],
      "source": [
        "input_shape_nopre = batch_x_train_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250d061b",
      "metadata": {
        "id": "250d061b"
      },
      "outputs": [],
      "source": [
        "# Architecture v4\n",
        "cnn4 = Sequential([       \n",
        "                   #layers.Input(input_shape[1:]),\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "\n",
        "# Builds the DAG\n",
        "cnn4.build(input_shape_nopre)\n",
        "# Check network\n",
        "cnn4.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb2b4de",
      "metadata": {
        "id": "fcb2b4de"
      },
      "outputs": [],
      "source": [
        "model=cnn4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f8793b",
      "metadata": {
        "id": "46f8793b"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2943c482",
      "metadata": {
        "id": "2943c482"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn4.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c77bf01",
      "metadata": {
        "id": "5c77bf01"
      },
      "outputs": [],
      "source": [
        "# logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# # Defining the basic TensorBoard callback.\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "# file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451bc8f5",
      "metadata": {
        "id": "451bc8f5"
      },
      "outputs": [],
      "source": [
        "# def log_confusion_matrix(epoch, logs):\n",
        "#     predictions_raw = model.predict(batch_x_val_scaled)\n",
        "#     predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "#     cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "#     figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "#     cm_image = plot_to_image(figure)\n",
        "\n",
        "#     with file_writer_cm.as_default():\n",
        "#         tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78241fb1",
      "metadata": {
        "id": "78241fb1"
      },
      "outputs": [],
      "source": [
        "# # Defining the per-epoch callback.\n",
        "# cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qsniq8NcPs2G",
      "metadata": {
        "id": "Qsniq8NcPs2G"
      },
      "outputs": [],
      "source": [
        "# Model training (v1)\n",
        "#Adding early stopping to help choosing the best number of epochs\n",
        "#code from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
        "\n",
        "cnn4.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])\n",
        "\n",
        "#Train\n",
        "history4 = cnn4.fit(ds_train_encoded,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=ds_val_encoded,\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint])#, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19cee3a2",
      "metadata": {
        "id": "19cee3a2"
      },
      "outputs": [],
      "source": [
        "# #Star tensorboard\n",
        "\n",
        "# %tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J37blBQoPs2R",
      "metadata": {
        "id": "J37blBQoPs2R"
      },
      "source": [
        "Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N-Kzndh4Ps2R",
      "metadata": {
        "id": "N-Kzndh4Ps2R"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist4 = pd.DataFrame.from_dict(history4.history)\n",
        "df_hist4[\"Epoch\"] = np.arange(1, len(df_hist4) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist4.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l8ZiRxJC9AeD",
      "metadata": {
        "id": "l8ZiRxJC9AeD"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv5\">\n",
        "\n",
        "## 3.5. CNN_V5, layers\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8803c7",
      "metadata": {
        "id": "5c8803c7"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f38b53",
      "metadata": {
        "id": "21f38b53"
      },
      "outputs": [],
      "source": [
        "# Architecture v5\n",
        "cnn5 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),                           \n",
        "                   # Feature extraction: first block\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: second block\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: third block\n",
        "                   layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: fourth (closing) block\n",
        "                   layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG \n",
        "cnn5.build(input_shape)\n",
        "# Check network\n",
        "cnn5.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbb3b88",
      "metadata": {
        "id": "ffbb3b88"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn5.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de468baa",
      "metadata": {
        "id": "de468baa"
      },
      "outputs": [],
      "source": [
        "model=cnn5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c884d9b3",
      "metadata": {
        "id": "c884d9b3"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0bd481",
      "metadata": {
        "id": "3d0bd481"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn5.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac454ed",
      "metadata": {
        "id": "9ac454ed"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a8530a",
      "metadata": {
        "id": "47a8530a"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c332337f",
      "metadata": {
        "id": "c332337f"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856d99ad",
      "metadata": {
        "id": "856d99ad"
      },
      "outputs": [],
      "source": [
        "# Model training (v5)\n",
        "\n",
        "#Train\n",
        "\n",
        "history5 = cnn5.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f7873f",
      "metadata": {
        "id": "f0f7873f"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57a7dc43",
      "metadata": {
        "id": "57a7dc43"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist5 = pd.DataFrame.from_dict(history5.history)\n",
        "df_hist5[\"Epoch\"] = np.arange(1, len(df_hist5) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax5 = df_hist5.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax5.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax5.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax5.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eWNdJSQ330_",
      "metadata": {
        "id": "3eWNdJSQ330_"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv6\">\n",
        "\n",
        "## 3.6. CNN_V6, drop-out\n",
        "\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60fa125d",
      "metadata": {
        "id": "60fa125d"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OU9iv6asqiv4",
      "metadata": {
        "id": "OU9iv6asqiv4"
      },
      "source": [
        "#### Drop-out 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3MUvav7X4TFA",
      "metadata": {
        "id": "3MUvav7X4TFA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
        "#https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
        "#drop-out rate: divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new \n",
        "#network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.\n",
        "\n",
        "# Architecture v5\n",
        "cnn6_1 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),                           \n",
        "                   # Feature extraction: first block\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: second block\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: third block\n",
        "                   layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: fourth (closing) block\n",
        "                   layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   #drop-out\n",
        "                   layers.Dropout(0.5), \n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG \n",
        "cnn6_1.build(input_shape)\n",
        "# Check network\n",
        "cnn6_1.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Lto9zG94TFB",
      "metadata": {
        "id": "-Lto9zG94TFB"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn6_1.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f791e384",
      "metadata": {
        "id": "f791e384"
      },
      "outputs": [],
      "source": [
        "model=cnn6_1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b1ed08",
      "metadata": {
        "id": "83b1ed08"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49d8770",
      "metadata": {
        "id": "b49d8770"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn6_1.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6e0bc7",
      "metadata": {
        "id": "dd6e0bc7"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da63c0d0",
      "metadata": {
        "id": "da63c0d0"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b805de22",
      "metadata": {
        "id": "b805de22"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_qH6CD8E4TFB",
      "metadata": {
        "id": "_qH6CD8E4TFB"
      },
      "outputs": [],
      "source": [
        "# Model training (v6)\n",
        "#Train\n",
        "\n",
        "history6__1 = cnn6_1.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e322601",
      "metadata": {
        "id": "2e322601"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DS269Z044TFB",
      "metadata": {
        "id": "DS269Z044TFB"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist6_1 = pd.DataFrame.from_dict(history6__1.history)\n",
        "df_hist6_1[\"Epoch\"] = np.arange(1, len(df_hist6_1) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax6_1 = df_hist6_1.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax6_1.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax6_1.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax6_1.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "teJEB-Taqwk_",
      "metadata": {
        "id": "teJEB-Taqwk_"
      },
      "source": [
        "#### Drop-out 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161b4b6e",
      "metadata": {
        "id": "161b4b6e"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q_SY-Fw3q4nT",
      "metadata": {
        "id": "q_SY-Fw3q4nT"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
        "#https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
        "#drop-out rate: divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new \n",
        "#network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.\n",
        "# Architecture v5\n",
        "cnn6_2 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),                           \n",
        "                   # Feature extraction: first block\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: second block\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: third block\n",
        "                   layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: fourth (closing) block\n",
        "                   layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   #drop-out\n",
        "                   layers.Dropout(0.8), \n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG \n",
        "cnn6_2.build(input_shape)\n",
        "# Check network\n",
        "cnn6_2.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48AHK7uoq4nT",
      "metadata": {
        "id": "48AHK7uoq4nT"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn6_2.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88256199",
      "metadata": {
        "id": "88256199"
      },
      "outputs": [],
      "source": [
        "model=cnn6_2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb5fdf8",
      "metadata": {
        "id": "9fb5fdf8"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a01ba9f",
      "metadata": {
        "id": "4a01ba9f"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn6_2.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49942377",
      "metadata": {
        "id": "49942377"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136147c0",
      "metadata": {
        "id": "136147c0"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a2bd6a",
      "metadata": {
        "id": "a9a2bd6a"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dgvd17vjq4nU",
      "metadata": {
        "id": "Dgvd17vjq4nU"
      },
      "outputs": [],
      "source": [
        "# Model training (v6)\n",
        "\n",
        "#Train\n",
        "\n",
        "history6_2 = cnn6_2.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b4ad36",
      "metadata": {
        "id": "58b4ad36"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UOoMry-eq4nU",
      "metadata": {
        "id": "UOoMry-eq4nU"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist6_2 = pd.DataFrame.from_dict(history6_2.history)\n",
        "df_hist6_2[\"Epoch\"] = np.arange(1, len(df_hist6_2) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax6_2 = df_hist6_2.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax6_2.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax6_2.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax6_2.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LmyE08xRq0yh",
      "metadata": {
        "id": "LmyE08xRq0yh"
      },
      "source": [
        " #### Drop-out 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e865e18f",
      "metadata": {
        "id": "e865e18f"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fnbJ8vnpq71D",
      "metadata": {
        "id": "fnbJ8vnpq71D"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
        "#https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
        "#drop-out rate: divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new \n",
        "#network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.\n",
        "\n",
        "# Architecture v5\n",
        "cnn6_3 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),                           \n",
        "                   # Feature extraction: first block\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: second block\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: third block\n",
        "                   layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: fourth (closing) block\n",
        "                   layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   #drop-out\n",
        "                   layers.Dropout(0.5), \n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG \n",
        "cnn6_3.build(input_shape)\n",
        "# Check network\n",
        "cnn6_3.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kShbw3Naq71D",
      "metadata": {
        "id": "kShbw3Naq71D"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn6_3.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce5623a",
      "metadata": {
        "id": "7ce5623a"
      },
      "outputs": [],
      "source": [
        "model=cnn6_3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e93c11",
      "metadata": {
        "id": "c9e93c11"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19e6925",
      "metadata": {
        "id": "a19e6925"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn6_3.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c32a478",
      "metadata": {
        "id": "4c32a478"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e122f5",
      "metadata": {
        "id": "b7e122f5"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eaa1724",
      "metadata": {
        "id": "9eaa1724"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VsJzVIHzq71E",
      "metadata": {
        "id": "VsJzVIHzq71E"
      },
      "outputs": [],
      "source": [
        "# Model training (v6)\n",
        "\n",
        "#Train\n",
        "\n",
        "history6_3 = cnn6_3.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a5bf9af",
      "metadata": {
        "id": "2a5bf9af"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PQs7OI2Pq71E",
      "metadata": {
        "id": "PQs7OI2Pq71E"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist6_3 = pd.DataFrame.from_dict(history6_3.history)\n",
        "df_hist6_3[\"Epoch\"] = np.arange(1, len(df_hist6_3) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax6_3 = df_hist6_3.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax6_3.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax6_3.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax6_3.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UeEZprbWiygK",
      "metadata": {
        "id": "UeEZprbWiygK"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv6\">\n",
        "\n",
        "## 3.6. CNN_V7, layer weight regularization\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b30fb1a",
      "metadata": {
        "id": "5b30fb1a"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qLKVx9cC2jw4",
      "metadata": {
        "id": "qLKVx9cC2jw4"
      },
      "outputs": [],
      "source": [
        "# Architecture v7\n",
        "\n",
        "cnn7 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),                           \n",
        "                   # Feature extraction: first block\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: second block\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: third block\n",
        "                   layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: fourth (closing) block\n",
        "                   layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                bias_regularizer=regularizers.L2(1e-4),\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG \n",
        "cnn7.build(input_shape)\n",
        "# Check network\n",
        "cnn7.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g1VIpe2U5-yK",
      "metadata": {
        "id": "g1VIpe2U5-yK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn7.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9d8625",
      "metadata": {
        "id": "eb9d8625"
      },
      "outputs": [],
      "source": [
        "model=cnn7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993d82b2",
      "metadata": {
        "id": "993d82b2"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef48c87",
      "metadata": {
        "id": "4ef48c87"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn7.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b229b7",
      "metadata": {
        "id": "07b229b7"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fea6329",
      "metadata": {
        "id": "4fea6329"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec6f3e9",
      "metadata": {
        "id": "6ec6f3e9"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3oegzT45-yV",
      "metadata": {
        "id": "c3oegzT45-yV"
      },
      "outputs": [],
      "source": [
        "# Model training (v7)\n",
        "\n",
        "#Train\n",
        "\n",
        "history7 = cnn7.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c641bb2",
      "metadata": {
        "id": "9c641bb2"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ptB299045-yV",
      "metadata": {
        "id": "ptB299045-yV"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist7 = pd.DataFrame.from_dict(history7.history)\n",
        "df_hist7[\"Epoch\"] = np.arange(1, len(df_hist7) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax7 = df_hist7.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax7.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax7.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax7.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a145163e",
      "metadata": {
        "id": "a145163e"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv4\">\n",
        "\n",
        "## 3.8. CNN_V8, Combining best methods\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc267395",
      "metadata": {
        "id": "fc267395"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2a9405",
      "metadata": {
        "id": "4f2a9405"
      },
      "source": [
        "#### Drop-out with Batch Normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173ad8b0",
      "metadata": {
        "id": "173ad8b0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
        "#https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
        "#drop-out rate: divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new \n",
        "#network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.\n",
        "\n",
        "# Architecture v6\n",
        "cnn8 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),                           \n",
        "                   # Feature extraction: first block\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: second block\n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: third block\n",
        "                   layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: fourth (closing) block\n",
        "                   layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   #drop-out\n",
        "                   layers.Dropout(0.5), \n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG \n",
        "cnn8.build(input_shape)\n",
        "# Check network\n",
        "cnn8.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed32771",
      "metadata": {
        "id": "fed32771"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn8.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94533ce",
      "metadata": {
        "id": "f94533ce"
      },
      "outputs": [],
      "source": [
        "model=cnn8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6bb37c3",
      "metadata": {
        "id": "c6bb37c3"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55abb377",
      "metadata": {
        "id": "55abb377"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn8.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07da80a0",
      "metadata": {
        "id": "07da80a0"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da957ed8",
      "metadata": {
        "id": "da957ed8"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "251958c2",
      "metadata": {
        "id": "251958c2"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d68d5d8",
      "metadata": {
        "id": "4d68d5d8"
      },
      "outputs": [],
      "source": [
        "# Model training (v5)\n",
        "#Train\n",
        "\n",
        "history8 = cnn8.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7eadbe2",
      "metadata": {
        "id": "b7eadbe2"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d672a5e",
      "metadata": {
        "id": "6d672a5e"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist8 = pd.DataFrame.from_dict(history8.history)\n",
        "df_hist8[\"Epoch\"] = np.arange(1, len(df_hist8) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax6 = df_hist8.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax6.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax6.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax6.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN8_2 (Batch Normalization + Layers + Dropout)"
      ],
      "metadata": {
        "id": "1QSMWSF5lVFu"
      },
      "id": "1QSMWSF5lVFu"
    },
    {
      "cell_type": "code",
      "source": [
        "cnn8_2 = Sequential([# The batch normalization layer \n",
        "                   layers.BatchNormalization(),                           \n",
        "                   # Feature extraction: first block\n",
        "                   layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: second block\n",
        "                   layers.BatchNormalization(),     \n",
        "                   layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: third block\n",
        "                   layers.BatchNormalization(),     \n",
        "                   layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),\n",
        "                   layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                   # Feature extraction: fourth (closing) block\n",
        "                   layers.BatchNormalization(),     \n",
        "                   layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                                 kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                 bias_regularizer=regularizers.L2(1e-4),\n",
        "                                 kernel_initializer=initializers.GlorotNormal(seed=seed)),\n",
        "                   layers.Activation(\"relu\"),                   \n",
        "                   layers.GlobalMaxPooling2D(),\n",
        "                   layers.Dropout(0.5),   \n",
        "                   #drop-out\n",
        "                   # Classification\n",
        "                   layers.Dense(units=n_classes, activation=\"softmax\",\n",
        "                                kernel_regularizer=regularizers.L2(1e-4),\n",
        "                                bias_regularizer=regularizers.L2(1e-4),\n",
        "                                kernel_initializer=initializers.GlorotNormal(seed=seed))])\n",
        "# Builds the DAG \n",
        "cnn8_2.build(input_shape)\n",
        "# Check network\n",
        "cnn8_2.summary()"
      ],
      "metadata": {
        "id": "Wc1CLf-5lcKK"
      },
      "id": "Wc1CLf-5lcKK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxl7bmRmoiQ6"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "cnn8_2.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])"
      ],
      "id": "Oxl7bmRmoiQ6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfVL0hx0oiQ6"
      },
      "outputs": [],
      "source": [
        "model=cnn8_2"
      ],
      "id": "OfVL0hx0oiQ6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqUtoF75oiQ7"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ],
      "id": "jqUtoF75oiQ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjiWYFmJoiQ7"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'cnn8_2.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ],
      "id": "VjiWYFmJoiQ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnJOjonMoiQ7"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ],
      "id": "OnJOjonMoiQ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGGqGoSHoiQ7"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ],
      "id": "RGGqGoSHoiQ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8jm7loQoiQ8"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ],
      "id": "n8jm7loQoiQ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRmmylJsoiQ8"
      },
      "outputs": [],
      "source": [
        "# Model training (v5)\n",
        "#Train\n",
        "\n",
        "history8_2 = cnn8_2.fit(ds_train_scaled,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    shuffle=shuffle,\n",
        "                    validation_data=(batch_x_val_scaled,batch_y_val_scaled),\n",
        "                    class_weight=weights,\n",
        "                    callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ],
      "id": "VRmmylJsoiQ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8fDQfQPoiQ8"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ],
      "id": "I8fDQfQPoiQ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj9tUUaLoiQ8"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist8_2 = pd.DataFrame.from_dict(history8_2.history)\n",
        "df_hist8_2[\"Epoch\"] = np.arange(1, len(df_hist8_2) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax6 = df_hist8_2.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax6.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax6.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax6.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ],
      "id": "Rj9tUUaLoiQ8"
    },
    {
      "cell_type": "markdown",
      "id": "KhWmqQa8eyG2",
      "metadata": {
        "id": "KhWmqQa8eyG2"
      },
      "source": [
        "[BACK TO TOC](#toc)\n",
        "    \n",
        "<a id='transferlearning'></a>\n",
        "\n",
        "# <font color = 'darkblue'> 4. Transferlearning </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uDUplF1QekWl",
      "metadata": {
        "id": "uDUplF1QekWl"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv4\">\n",
        "\n",
        "## 4.1. Transferlearning VGG16 model\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10eab7c4",
      "metadata": {
        "id": "10eab7c4"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "160c1f4d",
      "metadata": {
        "id": "160c1f4d"
      },
      "source": [
        "Inspired by: https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YI13LJNGe5fO",
      "metadata": {
        "id": "YI13LJNGe5fO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VNlVGMm8sRgB",
      "metadata": {
        "id": "VNlVGMm8sRgB"
      },
      "outputs": [],
      "source": [
        "## Loading VGG16 model\n",
        "input_shape_vgg=batch_x_train_scaled[0].shape\n",
        "vgg16_base = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape_vgg)\n",
        "vgg16_base.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GaqVCSvFhaVZ",
      "metadata": {
        "id": "GaqVCSvFhaVZ"
      },
      "outputs": [],
      "source": [
        "ds_train_vgg16 = ds_train_encoded.map(lambda x, y: (preprocess_input(x), y))\n",
        "batch_x_train_vgg16, batch_y_train_vgg16 = next(iter(ds_train_vgg16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2sArWMnrimjb",
      "metadata": {
        "id": "2sArWMnrimjb"
      },
      "outputs": [],
      "source": [
        "ds_val_vgg16 = ds_val_encoded.map(lambda x, y: (preprocess_input(x), y))\n",
        "batch_x_val_vgg16, batch_y_val_vgg16 = next(iter(ds_val_vgg16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJGrVy7SsRyk",
      "metadata": {
        "id": "yJGrVy7SsRyk"
      },
      "outputs": [],
      "source": [
        "vgg16_base.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JO1nnyUJe5oe",
      "metadata": {
        "id": "JO1nnyUJe5oe"
      },
      "outputs": [],
      "source": [
        "vgg16 = Sequential([vgg16_base,\n",
        "                    layers.Flatten(),\n",
        "                    layers.Dense(50, activation='relu'),\n",
        "                    layers.Dense(20, activation='relu'),\n",
        "                    layers.Dense(units=6, activation=\"softmax\")               \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4e42c9",
      "metadata": {
        "id": "7f4e42c9"
      },
      "outputs": [],
      "source": [
        "model=vgg16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398557c6",
      "metadata": {
        "id": "398557c6"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c302d0d",
      "metadata": {
        "id": "4c302d0d"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'vgg16.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388bb7a5",
      "metadata": {
        "id": "388bb7a5"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e9c52d",
      "metadata": {
        "id": "69e9c52d"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd1697d",
      "metadata": {
        "id": "9bd1697d"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uXG8nP41e5wv",
      "metadata": {
        "id": "uXG8nP41e5wv"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "vgg16.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "  metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'],\n",
        "  weighted_metrics= ['accuracy']\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HFvXt_qSvl-A",
      "metadata": {
        "id": "HFvXt_qSvl-A"
      },
      "outputs": [],
      "source": [
        "history_vgg16 = vgg16.fit(ds_train_vgg16 ,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle,\n",
        "                          epochs=epochs,\n",
        "                          validation_data=ds_val_vgg16,\n",
        "                          class_weight=weights,\n",
        "                          callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e828dafd",
      "metadata": {
        "id": "e828dafd"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9lNGhouRmuRS",
      "metadata": {
        "id": "9lNGhouRmuRS"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_histvgg = pd.DataFrame.from_dict(history_vgg16.history)\n",
        "df_histvgg[\"Epoch\"] = np.arange(1, len(df_histvgg) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist1.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ln1hLlos5xr1",
      "metadata": {
        "id": "ln1hLlos5xr1"
      },
      "source": [
        "<a class=\"anchor\" id=\"CNNv4\">\n",
        "\n",
        "## 4.2. Transferlearning ResNet50 model\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab09f73",
      "metadata": {
        "id": "1ab09f73"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "evJIY4Z0ocZo",
      "metadata": {
        "id": "evJIY4Z0ocZo"
      },
      "source": [
        "#### Preprocessing Resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HYBoDD6PDS5n",
      "metadata": {
        "id": "HYBoDD6PDS5n"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "ds_train_resnet = ds_train_encoded.map(lambda x, y: (preprocess_input(x), y))\n",
        "batch_x_train_resnet, batch_y_train_resnet = next(iter(ds_train_resnet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p1A_1y6-DS5o",
      "metadata": {
        "id": "p1A_1y6-DS5o"
      },
      "outputs": [],
      "source": [
        "ds_val_resnet = ds_val_encoded.map(lambda x, y: (preprocess_input(x), y))\n",
        "batch_x_val_resnet, batch_y_val_resnet = next(iter(ds_val_resnet))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HIb3xlRQlSP2",
      "metadata": {
        "id": "HIb3xlRQlSP2"
      },
      "source": [
        "#### First Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a45742d",
      "metadata": {
        "id": "9a45742d"
      },
      "outputs": [],
      "source": [
        "# Freeze the layers of the ResNet50 model\n",
        "\n",
        "resnet = ResNet50(include_top = False, input_shape = (128,128,3),weights = 'imagenet', classifier_activation = 'softmax')\n",
        "\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "resnet = Sequential([resnet,\n",
        "                   #layers.Flatten(),\n",
        "                   layers.GlobalAveragePooling2D(),\n",
        "                   layers.Dense(128, activation='relu'),\n",
        "                   layers.Dense(units = 6, activation='softmax'),\n",
        "])\n",
        "\n",
        "print(resnet.summary())\n",
        "\n",
        "resnet.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888411b4",
      "metadata": {
        "id": "888411b4"
      },
      "outputs": [],
      "source": [
        "model=resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efed8d4b",
      "metadata": {
        "id": "efed8d4b"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98fe62cd",
      "metadata": {
        "id": "98fe62cd"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'resnet.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909b35ee",
      "metadata": {
        "id": "909b35ee"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235137d5",
      "metadata": {
        "id": "235137d5"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c7b60b",
      "metadata": {
        "id": "64c7b60b"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wdTYoXZ-OXB2",
      "metadata": {
        "id": "wdTYoXZ-OXB2"
      },
      "outputs": [],
      "source": [
        "# Model training\n",
        "#Adding early stopping to help choosing the best number of epochs\n",
        "#code from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
        "\n",
        "#Train\n",
        "history_resnet = resnet.fit(ds_train_resnet , batch_size=batch_size, shuffle=shuffle, epochs=epochs, validation_data=ds_val_resnet, \n",
        "                          class_weight=weights, callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e1abb5",
      "metadata": {
        "id": "f5e1abb5"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vb8yYiqlOhC0",
      "metadata": {
        "id": "vb8yYiqlOhC0"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist_restnet = pd.DataFrame.from_dict(history_resnet.history)\n",
        "df_hist_restnet[\"Epoch\"] = np.arange(1, len(df_hist_restnet) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist_restnet.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa_ql34LlL5n",
      "metadata": {
        "id": "fa_ql34LlL5n"
      },
      "source": [
        "#### Testing layers -> Best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc38d86",
      "metadata": {
        "id": "7fc38d86"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d367c1",
      "metadata": {
        "id": "07d367c1"
      },
      "outputs": [],
      "source": [
        "resnet = ResNet50(include_top = False, input_shape = (128,128,3),weights = 'imagenet', classifier_activation = 'softmax')\n",
        "\n",
        "# Freeze the layers of the ResNet50 model\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "resnet_2 = Sequential([resnet,\n",
        "                   #layers.Flatten(),\n",
        "                   layers.GlobalAveragePooling2D(),\n",
        "                   layers.Dropout(0.2),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Dropout(0.2),\n",
        "                   layers.Dense(128, activation='relu'),\n",
        "                   layers.Dropout(0.1),\n",
        "                   layers.Dense(units = 6, activation='softmax')\n",
        "])\n",
        "\n",
        "print(resnet_2.summary())\n",
        "\n",
        "resnet_2.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d6ffc8",
      "metadata": {
        "id": "b3d6ffc8"
      },
      "outputs": [],
      "source": [
        "model=resnet_2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32327808",
      "metadata": {
        "id": "32327808"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd3cb5f",
      "metadata": {
        "id": "dbd3cb5f"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'resnet_2.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "545672a1",
      "metadata": {
        "id": "545672a1"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afef2488",
      "metadata": {
        "id": "afef2488"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d04f9d2c",
      "metadata": {
        "id": "d04f9d2c"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kEV7fLqMO7d1",
      "metadata": {
        "id": "kEV7fLqMO7d1"
      },
      "source": [
        "Might be underfitting since the lines are very flat\n",
        "According to Andrew Ng, the best methods of dealing with an underfitting model is trying a bigger neural network (adding new layers or increasing the number of neurons in existing layers) or training the model a little bit longer.\n",
        "https://www.mikulskibartosz.name/how-to-deal-with-underfitting-and-overfitting-in-deep-learning/#:~:text=According%20to%20Andrew%20Ng%2C%20the,model%20a%20little%20bit%20longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UC4pGG0xJfMO",
      "metadata": {
        "id": "UC4pGG0xJfMO"
      },
      "outputs": [],
      "source": [
        "#code inspired from: https://www.kaggle.com/code/imsparsh/food-classifier-mobilenetv2-resnet50-vgg16#Training-different-models\n",
        "\n",
        "\n",
        "# Model training (v1)\n",
        "#Adding early stopping to help choosing the best number of epochs\n",
        "#code from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
        "\n",
        "#Train\n",
        "history_resnet_2 = resnet_2.fit(ds_train_resnet , batch_size=batch_size, shuffle=shuffle, epochs=epochs, validation_data=ds_val_resnet, \n",
        "                          class_weight=weights, callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7104f7d",
      "metadata": {
        "id": "f7104f7d"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jtqaUbsfTDV5",
      "metadata": {
        "id": "jtqaUbsfTDV5"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist_restnet_2 = pd.DataFrame.from_dict(history_resnet_2.history)\n",
        "df_hist_restnet_2[\"Epoch\"] = np.arange(1, len(df_hist_restnet_2) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist_restnet_2.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7N2rncE6lDie",
      "metadata": {
        "id": "7N2rncE6lDie"
      },
      "source": [
        "#### Bigger DropOut -> Does not improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e34fd81",
      "metadata": {
        "id": "6e34fd81"
      },
      "outputs": [],
      "source": [
        "# #Clear previous logs\n",
        "\n",
        "# #collab\n",
        "!rm -rf /logs/ \n",
        "\n",
        "\n",
        "# #jupyter using Windows based on https://www.machinelearningnuggets.com/tensorboard-tutorial/\n",
        "# try:\n",
        "#     shutil.rmtree('logs')\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9444f1d4",
      "metadata": {
        "id": "9444f1d4"
      },
      "outputs": [],
      "source": [
        "#code inspired from: https://www.kaggle.com/code/imsparsh/food-classifier-mobilenetv2-resnet50-vgg16#Training-different-models\n",
        "\n",
        "resnet = ResNet50(include_top = False, input_shape = (128,128,3),weights = 'imagenet', classifier_activation = 'softmax')\n",
        "\n",
        "# Freeze the layers of the ResNet50 model\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "resnet_3 = Sequential([resnet,\n",
        "                   #layers.Flatten(),\n",
        "                   layers.GlobalAveragePooling2D(),\n",
        "                   layers.Dropout(0.8),\n",
        "                   layers.BatchNormalization(),\n",
        "                   layers.Dropout(0.5),\n",
        "                   layers.Dense(128, activation='relu'),\n",
        "                   layers.Dropout(0.1),\n",
        "                   layers.Dense(units = 6, activation='softmax')\n",
        "])\n",
        "\n",
        "print(resnet_3.summary())\n",
        "\n",
        "resnet_3.compile(\n",
        "  optimizer= optimizers.Adam(learning_rate=learning_rate),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "    metrics= [tfa.metrics.f_scores.F1Score(num_classes=6,average=\"weighted\", name = 'f1_score'), 'accuracy'], weighted_metrics= ['accuracy'])\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c0d75e",
      "metadata": {
        "id": "86c0d75e"
      },
      "outputs": [],
      "source": [
        "model=resnet_3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1738bd68",
      "metadata": {
        "id": "1738bd68"
      },
      "source": [
        "Callbacks API setup (inspired in the class code):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc72301a",
      "metadata": {
        "id": "dc72301a"
      },
      "outputs": [],
      "source": [
        "#checkpoint_filepath = '/content/drive/MyDrive/IMS_DeepLearning'\n",
        "checkpoint_filepath = 'resnet_3.weights.best.hdf6'\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=\"epoch\",\n",
        "        save_best_only=True)\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor=\"val_f1_score\",\n",
        "    min_delta=0,\n",
        "    mode=\"max\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50d8c9a7",
      "metadata": {
        "id": "50d8c9a7"
      },
      "outputs": [],
      "source": [
        "logdir = \"logs/image/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Defining the basic TensorBoard callback.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71473aa",
      "metadata": {
        "id": "a71473aa"
      },
      "outputs": [],
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    predictions_raw = model.predict(batch_x_val_scaled)\n",
        "    predictions = np.argmax(predictions_raw, axis=1)\n",
        "\n",
        "    cm = sklearn.metrics.confusion_matrix(np.argmax(batch_y_val_scaled, axis=1), predictions)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f1103f",
      "metadata": {
        "id": "a4f1103f"
      },
      "outputs": [],
      "source": [
        "# Defining the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8m0793-g27C",
      "metadata": {
        "id": "a8m0793-g27C"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Model training (v1)\n",
        "#Adding early stopping to help choosing the best number of epochs\n",
        "#code from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
        "\n",
        "#Train\n",
        "history_resnet_3 = resnet_3.fit(ds_train_resnet , batch_size=batch_size, shuffle=shuffle, epochs=epochs, validation_data=ds_val_resnet, \n",
        "                          class_weight=weights, callbacks =[early_stopping, model_checkpoint, tensorboard_callback, cm_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31b26d12",
      "metadata": {
        "id": "31b26d12"
      },
      "outputs": [],
      "source": [
        "#Star tensorboard\n",
        "\n",
        "%tensorboard --logdir logs/image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RHIN20zyg27D",
      "metadata": {
        "id": "RHIN20zyg27D"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame object\n",
        "df_hist_restnet_3 = pd.DataFrame.from_dict(history_resnet_3.history)\n",
        "df_hist_restnet_3[\"Epoch\"] = np.arange(1, len(df_hist_restnet_3) + 1, 1)\n",
        "# Plot learning curves\n",
        "secondary_y = [\"f1_score\", \"val_f1_score\"]\n",
        "ax = df_hist_restnet_3.plot(x=\"Epoch\", y=['loss', 'val_loss'] + secondary_y,\n",
        "                   secondary_y = secondary_y,\n",
        "                   kind=\"line\", figsize=(6, 3), grid=True, legend=True,\n",
        "                   ylabel=\"Cross-entropy\", \n",
        "                   xlabel=\"Epoch\", title=\"Learning curves\",                  \n",
        "                   color=['darkred', 'indianred', \"darkblue\", \"royalblue\"], alpha=0.75, fontsize=10)\n",
        "ax.right_ax.set_ylabel(\"Accuracy\")\n",
        "ax.right_ax.legend(loc=(0.25, -0.45), framealpha=1.0)\n",
        "ax.legend(loc=(0, -0.45), framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7KSEhvT9wXaL",
      "metadata": {
        "id": "7KSEhvT9wXaL"
      },
      "source": [
        "[BACK TO TOC](#toc)\n",
        "    \n",
        "<a id='transferlearning'></a>\n",
        "\n",
        "# <font color = 'darkblue'> 4. Saving the Models </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F7L5OouPwZmy",
      "metadata": {
        "id": "F7L5OouPwZmy"
      },
      "outputs": [],
      "source": [
        "cnn1.save('/content/drive/MyDrive/IMS_DeepLearning/Models')\n",
        "cnn2.save('/content/drive/MyDrive/IMS_DeepLearning/Models')\n",
        "cnn3.save('/content/drive/MyDrive/IMS_DeepLearning/Models')\n",
        "#cnn4.save('/content/drive/MyDrive/IMS_DeepLearning/Models')\n",
        "cnn5.save('/content/drive/MyDrive/IMS_DeepLearning/Models')\n",
        "cnn6.save('/content/drive/MyDrive/IMS_DeepLearning/Models')\n",
        "cnn7.save('/content/drive/MyDrive/IMS_DeepLearning/Models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pZ6MVcAfzBLs",
      "metadata": {
        "id": "pZ6MVcAfzBLs"
      },
      "outputs": [],
      "source": [
        "vgg16.save('vgg16','/content/drive/MyDrive/IMS_DeepLearning/Models')\n",
        "resnet.save('resnet','/content/drive/MyDrive/IMS_DeepLearning/Models')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tbn-N5bwxuSn",
      "metadata": {
        "id": "Tbn-N5bwxuSn"
      },
      "source": [
        "[BACK TO TOC](#toc)\n",
        "    \n",
        "<a id='transferlearning'></a>\n",
        "\n",
        "# <font color = 'darkblue'> 4. Testing with Test dataset </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jBdpHG6m2GBm",
      "metadata": {
        "id": "jBdpHG6m2GBm"
      },
      "source": [
        "<a class=\"anchor\" id=\"Datapreprocessing\">\n",
        "\n",
        "## Importing \n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zFCL5RYWxyd8",
      "metadata": {
        "id": "zFCL5RYWxyd8"
      },
      "outputs": [],
      "source": [
        "data_dir_test = '/content/drive/MyDrive/IMS_DeepLearning/dataset_dishes/test'\n",
        "ds_test = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir_test,\n",
        "  image_size=image_size,\n",
        "    crop_to_aspect_ratio=crop_to_aspect_ratio,\n",
        "    color_mode=color_mode,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=shuffle,\n",
        "    seed=seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5yf0mv2G0Vpc",
      "metadata": {
        "id": "5yf0mv2G0Vpc"
      },
      "outputs": [],
      "source": [
        "#img = ds_test[0]\n",
        "# img = Image.fromarray(img, 'RGB')\n",
        "# img.save('outfile.jpg')\n",
        "# cv2.imwrite('myImage.png',img)\n",
        "\n",
        "# img = image.load_img('outfile.jpg', target_size=(200, 200))\n",
        "# img_tensor = image.img_to_array(img)\n",
        "#img_tensor = np.expand_dims(img, axis=0)\n",
        "#img_tensor /= 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5VNuN4Zx0z-k",
      "metadata": {
        "id": "5VNuN4Zx0z-k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "WDI44FPL00Hf",
      "metadata": {
        "id": "WDI44FPL00Hf"
      },
      "source": [
        "<a class=\"anchor\" id=\"Datapreprocessing\">\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "op1d0saY00Hg",
      "metadata": {
        "id": "op1d0saY00Hg"
      },
      "source": [
        "Encoding the classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usOs-trV00Hg",
      "metadata": {
        "id": "usOs-trV00Hg"
      },
      "outputs": [],
      "source": [
        "ds_test_encoded = ds_test.map(lambda x, y: (x, encoding(y)))\n",
        "batch_x_test_encoded, batch_y_test_encoded = next(iter(ds_test_encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w7E3UxF000Hg",
      "metadata": {
        "id": "w7E3UxF000Hg"
      },
      "outputs": [],
      "source": [
        "rescaling = layers.Rescaling(1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qNwxTuWt00Hg",
      "metadata": {
        "id": "qNwxTuWt00Hg"
      },
      "outputs": [],
      "source": [
        "ds_test_scaled = ds_test_encoded.map(lambda x, y: (rescaling(x), y))\n",
        "batch_x_test_scaled, batch_y_test_scaled = next(iter(ds_test_scaled))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fvinq4jE1c9O",
      "metadata": {
        "id": "fvinq4jE1c9O"
      },
      "source": [
        "<a class=\"anchor\" id=\"Datapreprocessing\">\n",
        "\n",
        "## Results with CNN1\n",
        "\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mMFcbAcnzuBu",
      "metadata": {
        "id": "mMFcbAcnzuBu"
      },
      "outputs": [],
      "source": [
        "img = batch_x_test_encoded[0]\n",
        "\n",
        "#predict = (vgg16.predict(img))\n",
        "\n",
        "cnn1.evaluate(batch_x_test_scaled, batch_y_test_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33984383",
      "metadata": {
        "id": "33984383"
      },
      "outputs": [],
      "source": [
        "classes=batch_y_test_scaled.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896d39c8",
      "metadata": {
        "id": "896d39c8"
      },
      "outputs": [],
      "source": [
        "classes=np.argmax(classes, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb7e040",
      "metadata": {
        "id": "cfb7e040"
      },
      "outputs": [],
      "source": [
        "cnn1.load_weights('cnn1.weights.best.hdf6')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ufjWzA_O2NrP",
      "metadata": {
        "id": "ufjWzA_O2NrP"
      },
      "outputs": [],
      "source": [
        "predicts= cnn1.predict(ds_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780e15c4",
      "metadata": {
        "id": "780e15c4"
      },
      "outputs": [],
      "source": [
        "predict_classes=np.argmax(predicts, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff848280",
      "metadata": {
        "id": "ff848280"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59aef3a8",
      "metadata": {
        "id": "59aef3a8"
      },
      "outputs": [],
      "source": [
        "acc= accuracy_score(classes,predict_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee148ac",
      "metadata": {
        "id": "4ee148ac"
      },
      "outputs": [],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313b83d5",
      "metadata": {
        "id": "313b83d5"
      },
      "outputs": [],
      "source": [
        "predict_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vJu7JH8Jn70h",
      "metadata": {
        "id": "vJu7JH8Jn70h"
      },
      "source": [
        "## Results with ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unONqnR9orqz",
      "metadata": {
        "id": "unONqnR9orqz"
      },
      "outputs": [],
      "source": [
        "ds_test_resnet = ds_test_encoded.map(lambda x, y: (preprocess_input(x), y))\n",
        "batch_x_test_resnet, batch_y_test_resnet = next(iter(ds_test_resnet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NIVPjm6Qo-5x",
      "metadata": {
        "id": "NIVPjm6Qo-5x"
      },
      "outputs": [],
      "source": [
        "img = batch_x_test_resnet[0]\n",
        "\n",
        "#predict = (vgg16.predict(img))\n",
        "\n",
        "resnet_2.evaluate(batch_x_test_resnet, batch_y_test_resnet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pk7YkRELo-5z",
      "metadata": {
        "id": "pk7YkRELo-5z"
      },
      "outputs": [],
      "source": [
        "classes=batch_y_test_resnet.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HhgDdl4xo-5z",
      "metadata": {
        "id": "HhgDdl4xo-5z"
      },
      "outputs": [],
      "source": [
        "classes=np.argmax(classes, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s54c7dY3o-5z",
      "metadata": {
        "id": "s54c7dY3o-5z"
      },
      "outputs": [],
      "source": [
        "#resnet_2.load_weights('rest.weights.best.hdf6')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "npBrAAxOo-5z",
      "metadata": {
        "id": "npBrAAxOo-5z"
      },
      "outputs": [],
      "source": [
        "predicts= resnet_2.predict(ds_test_resnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A4Iqw_MDo-5z",
      "metadata": {
        "id": "A4Iqw_MDo-5z"
      },
      "outputs": [],
      "source": [
        "predict_classes=np.argmax(predicts, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wbWO1Skpo-5z",
      "metadata": {
        "id": "wbWO1Skpo-5z"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OQZ2Kef8o-5z",
      "metadata": {
        "id": "OQZ2Kef8o-5z"
      },
      "outputs": [],
      "source": [
        "acc= accuracy_score(classes,predict_classes)\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F2uSBh9ir5Va",
      "metadata": {
        "id": "F2uSBh9ir5Va"
      },
      "outputs": [],
      "source": [
        "print('Classes:',classes)\n",
        "print('Predict:',predict_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN2"
      ],
      "metadata": {
        "id": "YqnzEuAZCcyp"
      },
      "id": "YqnzEuAZCcyp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu1c0UO5Cmhv"
      },
      "outputs": [],
      "source": [
        "img = batch_x_test_encoded[0]\n",
        "\n",
        "#predict = (vgg16.predict(img))\n",
        "\n",
        "cnn_2.evaluate(batch_x_test_scaled, batch_y_test_scaled)\n"
      ],
      "id": "yu1c0UO5Cmhv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyfGyvnLCmhw"
      },
      "outputs": [],
      "source": [
        "classes=batch_y_test_scaled.numpy()"
      ],
      "id": "lyfGyvnLCmhw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywvA1FL0Cmhw"
      },
      "outputs": [],
      "source": [
        "classes=np.argmax(classes, axis=1)"
      ],
      "id": "ywvA1FL0Cmhw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxDZ0wfUCmhw"
      },
      "outputs": [],
      "source": [
        "cnn2.load_weights('cnn2.weights.best.hdf6')"
      ],
      "id": "JxDZ0wfUCmhw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMUdavsBCmhw"
      },
      "outputs": [],
      "source": [
        "predicts= cnn2.predict(ds_test_scaled)"
      ],
      "id": "UMUdavsBCmhw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-7-DPs9Cmhw"
      },
      "outputs": [],
      "source": [
        "predict_classes=np.argmax(predicts, axis=1)"
      ],
      "id": "3-7-DPs9Cmhw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NuH-gtgCmhx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "3NuH-gtgCmhx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYrH5R3KCmhx"
      },
      "outputs": [],
      "source": [
        "acc= accuracy_score(classes,predict_classes)"
      ],
      "id": "kYrH5R3KCmhx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBe5pAhICmhx"
      },
      "outputs": [],
      "source": [
        "classes"
      ],
      "id": "GBe5pAhICmhx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32kkcHpmCmhx"
      },
      "outputs": [],
      "source": [
        "predict_classes"
      ],
      "id": "32kkcHpmCmhx"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CtrZ_j0rUHd"
      },
      "id": "-CtrZ_j0rUHd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_it3OidArcio"
      },
      "id": "_it3OidArcio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados Modelos"
      ],
      "metadata": {
        "id": "2FnZgBLurfjF"
      },
      "id": "2FnZgBLurfjF"
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn1.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn1.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn1.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "XOv-0YN1rgwA"
      },
      "id": "XOv-0YN1rgwA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn2_1.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn2_1.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn2_1.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "q7A1MpWPrjet"
      },
      "id": "q7A1MpWPrjet",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn2_2.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn2_2.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn2_2.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "6orroGpGsB9H"
      },
      "id": "6orroGpGsB9H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn3.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn3.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn3.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "TaziJMJ4rkOG"
      },
      "id": "TaziJMJ4rkOG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn4.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn4.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn4.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "59yW7foLrky8"
      },
      "id": "59yW7foLrky8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn5.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn5.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn5.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "xGKRbL9prlcu"
      },
      "id": "xGKRbL9prlcu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn6_1.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn6_1.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn6_1.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "IkshidQ-rl-9"
      },
      "id": "IkshidQ-rl-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn6_2.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn6_2.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn6_2.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "1JeHcJS4sFcM"
      },
      "id": "1JeHcJS4sFcM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn7.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn7.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn7.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "XvAYv1fMrnjt"
      },
      "id": "XvAYv1fMrnjt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = cnn8.evaluate(batch_x_train_scaled,batch_y_train_scaled, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn8.evaluate(batch_x_val_scaled,batch_y_val_scaled, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = cnn8.evaluate(batch_x_test_scaled,batch_y_test_scaled, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "Ukb7zOR1roDs"
      },
      "id": "Ukb7zOR1roDs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ],
      "metadata": {
        "id": "gTTosCBD5Bnz"
      },
      "id": "gTTosCBD5Bnz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test_vgg16 = ds_test_encoded.map(lambda x, y: (preprocess_input(x), y))\n",
        "batch_x_test_vgg16, batch_y_test_vgg16 = next(iter(ds_test_vgg16))"
      ],
      "metadata": {
        "id": "OYL5eo6r4H1S"
      },
      "id": "OYL5eo6r4H1S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = vgg16.evaluate(batch_x_train_vgg16,batch_y_train_vgg16, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = vgg16.evaluate(batch_x_val_vgg16,batch_y_val_vgg16, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = vgg16.evaluate(batch_x_test_vgg16,batch_y_test_vgg16, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "T3qyjUhy0M-0"
      },
      "id": "T3qyjUhy0M-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CKdf_gq5UVm"
      },
      "id": "8CKdf_gq5UVm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet import preprocess_input"
      ],
      "metadata": {
        "id": "b3Gvy80_5UvY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b3Gvy80_5UvY"
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test_resnet = ds_test_encoded.map(lambda x, y: (preprocess_input(x), y))\n",
        "batch_x_test_resnet, batch_y_test_resnet = next(iter(ds_test_resnet))"
      ],
      "metadata": {
        "id": "e26mbo9O5UvY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "e26mbo9O5UvY"
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = resnet.evaluate(batch_x_train_resnet,batch_y_train_resnet, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = resnet.evaluate(batch_x_val_resnet,batch_y_val_resnet, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = resnet.evaluate(batch_x_test_resnet,batch_y_test_resnet, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "NL5FIcgz0Ny_"
      },
      "id": "NL5FIcgz0Ny_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, f1_score, acc, weighted_accuracy = resnet_2.evaluate(batch_x_train_resnet,batch_y_train_resnet, verbose=0)\n",
        "\n",
        "print(\"Training loss score: %0.3f\" % (score))\n",
        "print(\"Training f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Training accuracy: %0.3f\" % (acc))\n",
        "print(\"Training weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = resnet_2.evaluate(batch_x_val_resnet,batch_y_val_resnet, verbose=0)\n",
        "\n",
        "print(\"Validation loss score: %0.3f\" % (score))\n",
        "print(\"Validation f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Validation accuracy: %0.3f\" % (acc))\n",
        "print(\"Validation weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "score, f1_score, acc, weighted_accuracy = resnet_2.evaluate(batch_x_test_resnet,batch_y_test_resnet, verbose=0)\n",
        "\n",
        "print(\"Test loss score: %0.3f\" % (score))\n",
        "print(\"Test f1-score: %0.3f\" % (f1_score))\n",
        "print(\"Test accuracy: %0.3f\" % (acc))\n",
        "print(\"Test weighted_accuracy: %0.3f\" % (weighted_accuracy))\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "vFcs84la0Wg7"
      },
      "id": "vFcs84la0Wg7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_2.evaluate(batch_x_test_resnet, batch_y_test_resnet)"
      ],
      "metadata": {
        "id": "Gwcn8lXO2bFr"
      },
      "id": "Gwcn8lXO2bFr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4a149aa4",
        "555260b7",
        "0e077650",
        "ccc5386c",
        "014367d1",
        "A_Tl191TpquY",
        "aba17414",
        "81974e5f",
        "744709bc",
        "ed47742f",
        "l8ZiRxJC9AeD",
        "OU9iv6asqiv4",
        "teJEB-Taqwk_",
        "LmyE08xRq0yh",
        "UeEZprbWiygK",
        "a145163e",
        "uDUplF1QekWl",
        "evJIY4Z0ocZo",
        "HIb3xlRQlSP2",
        "fa_ql34LlL5n",
        "7N2rncE6lDie",
        "7KSEhvT9wXaL",
        "jBdpHG6m2GBm",
        "WDI44FPL00Hf",
        "vJu7JH8Jn70h"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}